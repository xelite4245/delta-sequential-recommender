{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7a132d",
   "metadata": {},
   "source": [
    "# Gym Trainer ML: Predicting Top Set Intensity\n",
    "\n",
    "This notebook demonstrates the workflow for predicting the optimal top set intensity for a user’s next workout session. \n",
    "We use past workout data, engineered features, and XGBoost to make predictions. \n",
    "\n",
    "**Goals:**\n",
    "- Clean and preprocess the workout data\n",
    "- Feature engineering for ML and rule-based logic\n",
    "- Train an XGBoost model\n",
    "- Keep rule-based features separate for UI/alerts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e7267",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "849c6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#skklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utils for ML (DE_utils.py)\n",
    "import de_utils as de_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5583a6",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "In this section, we load the workout data and perform an initial inspection to understand the dataset. This includes checking:\n",
    "\n",
    "- The number of rows and columns.\n",
    "- Basic statistics about the numerical columns.\n",
    "- Data types and potential issues (e.g., missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfcd8f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13921 entries, 0 to 13920\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   date                 13921 non-null  object \n",
      " 1   distance             13921 non-null  float64\n",
      " 2   effective_load       13921 non-null  float64\n",
      " 3   exercise_name        13921 non-null  object \n",
      " 4   exercise_normalized  13921 non-null  object \n",
      " 5   notes                42 non-null     object \n",
      " 6   reps                 13921 non-null  int64  \n",
      " 7   rpe                  8426 non-null   float64\n",
      " 8   seconds              13921 non-null  int64  \n",
      " 9   set_order            13921 non-null  int64  \n",
      " 10  set_volume           13921 non-null  float64\n",
      " 11  weight               13921 non-null  float64\n",
      " 12  workout_name         13921 non-null  object \n",
      " 13  workout_notes        221 non-null    object \n",
      "dtypes: float64(5), int64(3), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>effective_load</th>\n",
       "      <th>reps</th>\n",
       "      <th>rpe</th>\n",
       "      <th>seconds</th>\n",
       "      <th>set_order</th>\n",
       "      <th>set_volume</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13921.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "      <td>8426.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "      <td>13921.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.063505</td>\n",
       "      <td>78.980662</td>\n",
       "      <td>8.931111</td>\n",
       "      <td>9.373645</td>\n",
       "      <td>0.769557</td>\n",
       "      <td>2.840816</td>\n",
       "      <td>1059.633528</td>\n",
       "      <td>132.519780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.457465</td>\n",
       "      <td>112.363625</td>\n",
       "      <td>3.277108</td>\n",
       "      <td>0.622712</td>\n",
       "      <td>24.986785</td>\n",
       "      <td>1.818789</td>\n",
       "      <td>971.775131</td>\n",
       "      <td>117.887481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.904110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>348.750000</td>\n",
       "      <td>35.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>138.013699</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1610.000000</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>363.000000</td>\n",
       "      <td>2632.054795</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1260.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17736.000000</td>\n",
       "      <td>2956.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distance  effective_load          reps          rpe       seconds  \\\n",
       "count  13921.000000    13921.000000  13921.000000  8426.000000  13921.000000   \n",
       "mean       0.063505       78.980662      8.931111     9.373645      0.769557   \n",
       "std        4.457465      112.363625      3.277108     0.622712     24.986785   \n",
       "min        0.000000        0.000000      0.000000     6.000000      0.000000   \n",
       "25%        0.000000        0.000000      6.000000     8.904110      0.000000   \n",
       "50%        0.000000       25.500000      8.000000     9.500000      0.000000   \n",
       "75%        0.000000      138.013699     10.000000    10.000000      0.000000   \n",
       "max      363.000000     2632.054795     60.000000    10.000000   1260.000000   \n",
       "\n",
       "          set_order    set_volume        weight  \n",
       "count  13921.000000  13921.000000  13921.000000  \n",
       "mean       2.840816   1059.633528    132.519780  \n",
       "std        1.818789    971.775131    117.887481  \n",
       "min        1.000000      0.000000      0.000000  \n",
       "25%        1.000000    348.750000     35.500000  \n",
       "50%        2.000000    900.000000    100.000000  \n",
       "75%        4.000000   1610.000000    225.000000  \n",
       "max       11.000000  17736.000000   2956.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Resolve the data file from common relative locations\n",
    "path = Path(\"../../data/processed/baseline_all_processed.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "X, y = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# View the basic info and statistics of the DataFrame\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "# Uncomment to see first 5 rows\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce214d",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "We drop the following columns, which are not relevant to predicting workout intensity:\n",
    "\n",
    "- `notes`, `workout_notes`: Textual notes about the workout that are not useful for prediction.\n",
    "- `exercise_name`, `seconds`: Columns that don’t contain relevant numerical or categorical data for this model.\n",
    "\n",
    "\n",
    "Also, this application focuses exclusively on strength training, so we remove any cardio-related entries based on the presence of distance values:\n",
    "Cardio exercises are identified by the presence of the `distance` column. Since we are only interested in strength training, we remove all rows where `distance` is NaN. Then we remove the cardio column altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22593402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows or columns we know are unnecessary for our ML application\n",
    "df = df.drop(columns=['notes', 'workout_notes', 'seconds'])\n",
    "\n",
    "# Drop cardio rows since we are focusing on strength training\n",
    "cardioRows = df[df['distance'].isna()]\n",
    "df = df.drop(cardioRows.index)\n",
    "\n",
    "# Drop distance column since it is now redundant\n",
    "df = df.drop(columns=['distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e066c",
   "metadata": {},
   "source": [
    "### View Missing values and Data types\n",
    "\n",
    "We now check for missing values across all columns. This will help us identify any columns that need imputation or handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b851841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "date                      0\n",
      "effective_load            0\n",
      "exercise_name             0\n",
      "exercise_normalized       0\n",
      "reps                      0\n",
      "rpe                    5495\n",
      "set_order                 0\n",
      "set_volume                0\n",
      "weight                    0\n",
      "workout_name              0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      "date                    object\n",
      "effective_load         float64\n",
      "exercise_name           object\n",
      "exercise_normalized     object\n",
      "reps                     int64\n",
      "rpe                    float64\n",
      "set_order                int64\n",
      "set_volume             float64\n",
      "weight                 float64\n",
      "workout_name            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecef74",
   "metadata": {},
   "source": [
    "### View Categorical and Numerical Columns\n",
    "\n",
    "Categorical columns with low cardinality are those with a small number of unique values. These columns are typically suitable for encoding using one-hot encoding or label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e419ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Categorical Columns with low cardinality:\n",
      "\n",
      "\n",
      "Unique Values for Categorical Columns with high cardinality:\n",
      "date\n",
      "exercise_name\n",
      "exercise_normalized\n",
      "workout_name\n",
      "\n",
      "\n",
      "Numerical Columns\n",
      "effective_load\n",
      "reps\n",
      "rpe\n",
      "set_order\n",
      "set_volume\n",
      "weight\n"
     ]
    }
   ],
   "source": [
    "def explore_columns(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    print(\"\\n\\nCategorical Columns with low cardinality:\")\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() <= 20:\n",
    "            print(f\"{col}\")\n",
    "            print(df[col].unique())\n",
    "            print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\\nUnique Values for Categorical Columns with high cardinality:\")\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() > 20:\n",
    "            print(f\"{col}\")\n",
    "\n",
    "    print(\"\\n\\nNumerical Columns\")\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        print(f\"{col}\")\n",
    "\n",
    "explore_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bb905",
   "metadata": {},
   "source": [
    "We need to make decisions about how to handle the missing values in the following columns:\n",
    "\n",
    "- **`notes`**: Drop this column, as it doesn't provide relevant information for our model.\n",
    "- **`rpe`**: Impute missing RPE values with the **median** (or mean), as RPE is a key feature for our model. Alternatively, if the proportion of missing values is significant, we could consider dropping the rows.\n",
    "- **`workout_notes`**: Drop this column, as it doesn't contribute to predicting workout intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6868d",
   "metadata": {},
   "source": [
    "Lets think about what features we truly need for a model that predicts the optimal top set intensity for the next session.\n",
    "What we definitely need:\n",
    "- `date` (for time-based features)\n",
    "- `effective_load` (target variable)\n",
    "- `reps`\n",
    "- `rpe` (after imputation and encoding)\n",
    "\n",
    "What we don't need:\n",
    "- `notes`\n",
    "- `workout_notes`\n",
    "- `exercise_name`\n",
    "- `workout_name`\n",
    "- `exercise_normalized`\n",
    "- `distance` (cardio rows can be dropped entirely)\n",
    "- `seconds`\n",
    "\n",
    "\n",
    "## Now for some Feature Engineering Brainstorming!\n",
    "\n",
    "#### Time-Based Feature Engineering\n",
    "- days_since_last_session: an integer indicating the number of days since the first workout\n",
    "- days_since_first_workout: an integer indicating the number of days since the last workout session\n",
    "- session_number: an integer indicating the session number for each exercise\n",
    "- rolling_avg_load_last_n_sessions: an integer indicating the rolling average load over the last n sessions\n",
    "- rolling_trend_load: an integer indicating the trend (slope) of load over the last n sessions\n",
    "\n",
    "#### RPE Feature Engineering\n",
    "First, we need to handle missing RPE values.\n",
    "Then we create a column indicating which columns are missing RPE values.\n",
    "Then we impute with median RPE.\n",
    "\n",
    "We'll bin RPE and then encode it as ordinal categorical variable.\n",
    "0 - 5 : Low\n",
    "6 - 7 : Medium\n",
    "8 - 10: High\n",
    "\n",
    "#### REPS Feature Engineering\n",
    "We can create bins for reps as well.\n",
    "rep_range_buckets:\n",
    "1-5: Strength\n",
    "6-15: Hypertrophy\n",
    "15+: Endurance\n",
    "\n",
    "<br>\n",
    "\n",
    ">Fortunately the functions to handle these feature engineering ideas have been defined in DE_utils.py, so no need to code it out. Just run the below cell and view the MI score for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed5a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Categorical Columns with low cardinality:\n",
      "\n",
      "\n",
      "Unique Values for Categorical Columns with high cardinality:\n",
      "exercise_name\n",
      "\n",
      "\n",
      "Numerical Columns\n",
      "effective_load\n",
      "reps\n",
      "set_order\n",
      "set_volume\n",
      "weight\n",
      "session_number\n",
      "days_since_first_workout\n",
      "days_since_last_workout\n",
      "rolling_avg_load_last_3_sessions\n",
      "rolling_trend_load\n",
      "rpe_missing\n",
      "rpe_ordinal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def feature_engineering_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main pipeline for feature engineering.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The raw DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    dfCopy = df.copy()\n",
    "\n",
    "\n",
    "    dfCopy = de_utils.add_session_number_per_exercise(dfCopy)\n",
    "    dfCopy = de_utils.add_time_features(dfCopy)\n",
    "    dfCopy = de_utils.add_days_since_last_workout(dfCopy)\n",
    "    dfCopy = de_utils.add_rolling_avg_load_last_n_sessions(dfCopy, n=3)\n",
    "    dfCopy = de_utils.add_rolling_trend_load(dfCopy, n=3)\n",
    "    dfCopy['rolling_trend_load'] = dfCopy['rolling_trend_load'].fillna(dfCopy['rolling_trend_load'].mean())\n",
    "\n",
    "    dfCopy = de_utils.handle_missing_rpe(dfCopy)\n",
    "    dfCopy = de_utils.bin_rpe(dfCopy)\n",
    "    dfCopy = de_utils.encode_rpe_ordinal(dfCopy)\n",
    "\n",
    "    # IMPORTANT: use dfCopy, not df, to ensure engineered columns are available\n",
    "    dfCopy = de_utils.filter_top_set_sessions(dfCopy)\n",
    "\n",
    "    # We can drop these columns since we already extracted useful features from them, and we get to keep our numerical features\n",
    "    dfCopy = dfCopy.drop(columns=['date', 'rpe', 'rpe_binned', 'workout_name', 'exercise_name', 'exercise_normalized'])\n",
    "  \n",
    "\n",
    "    return dfCopy\n",
    "\n",
    "\n",
    "\n",
    "df = feature_engineering_pipeline(df)\n",
    "\n",
    "explore_columns(df)\n",
    "\n",
    "# mi_scores = de_utils.get_mi_scores(X, y)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nMutual Information Scores:\")\n",
    "# print(mi_scores.sort_values(ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7a132d",
   "metadata": {},
   "source": [
    "# Gym Trainer ML: Predicting Top Set Intensity\n",
    "\n",
    "This notebook demonstrates the workflow for predicting the optimal top set intensity for a user’s next workout session. \n",
    "We use past workout data, engineered features, and XGBoost to make predictions. \n",
    "\n",
    "**Goals:**\n",
    "- Clean and preprocess the workout data\n",
    "- Feature engineering for ML and rule-based logic\n",
    "- Train an XGBoost model\n",
    "- Keep rule-based features separate for UI/alerts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e7267",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#skklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utils for ML (DE_utils.py)\n",
    "import de_utils as de_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5583a6",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "In this section, we load the workout data and perform an initial inspection to understand the dataset. This includes checking:\n",
    "\n",
    "- The number of rows and columns.\n",
    "- Basic statistics about the numerical columns.\n",
    "- Data types and potential issues (e.g., missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Resolve the data file from common relative locations\n",
    "path = Path(\"../../data/processed/baseline_all_processed.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# View the basic info and statistics of the DataFrame\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "# Uncomment to see first 5 rows\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce214d",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "We drop the following columns, which are not relevant to predicting workout intensity:\n",
    "\n",
    "- `notes`, `workout_notes`: Textual notes about the workout that are not useful for prediction.\n",
    "- `exercise_name`, `seconds`: Columns that don’t contain relevant numerical or categorical data for this model.\n",
    "\n",
    "\n",
    "Also, this application focuses exclusively on strength training, so we remove any cardio-related entries based on the presence of distance values:\n",
    "Cardio exercises are identified by the presence of the `distance` column. Since we are only interested in strength training, we remove all rows where `distance` is NaN. Then we remove the cardio column altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22593402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows or columns we know are unnecessary for our ML application\n",
    "df = df.drop(columns=['notes', 'workout_notes', 'seconds'])\n",
    "\n",
    "# Drop cardio rows since we are focusing on strength training\n",
    "cardioRows = df[df['distance'].isna()]\n",
    "df = df.drop(cardioRows.index)\n",
    "\n",
    "# Drop distance column since it is now redundant\n",
    "df = df.drop(columns=['distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e066c",
   "metadata": {},
   "source": [
    "### View Missing values and Data types\n",
    "\n",
    "We now check for missing values across all columns. This will help us identify any columns that need imputation or handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b851841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecef74",
   "metadata": {},
   "source": [
    "### View Categorical and Numerical Columns\n",
    "\n",
    "Categorical columns with low cardinality are those with a small number of unique values. These columns are typically suitable for encoding using one-hot encoding or label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_columns(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    print(\"\\n\\nCategorical Columns with low cardinality:\")\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() <= 20:\n",
    "            print(f\"{col}\")\n",
    "            print(df[col].unique())\n",
    "            print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\\nUnique Values for Categorical Columns with high cardinality:\")\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() > 20:\n",
    "            print(f\"{col}\")\n",
    "\n",
    "    print(\"\\n\\nNumerical Columns\")\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        print(f\"{col}\")\n",
    "\n",
    "explore_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bb905",
   "metadata": {},
   "source": [
    "We need to make decisions about how to handle the missing values in the following columns:\n",
    "\n",
    "- **`notes`**: Drop this column, as it doesn't provide relevant information for our model.\n",
    "- **`rpe`**: Impute missing RPE values with the **median** (or mean), as RPE is a key feature for our model. Alternatively, if the proportion of missing values is significant, we could consider dropping the rows.\n",
    "- **`workout_notes`**: Drop this column, as it doesn't contribute to predicting workout intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6868d",
   "metadata": {},
   "source": [
    "Lets think about what features we truly need for a model that predicts the optimal top set intensity for the next session.\n",
    "What we definitely need:\n",
    "- `date` (for time-based features)\n",
    "- `effective_load` (target variable)\n",
    "- `reps`\n",
    "- `rpe` (after imputation and encoding)\n",
    "\n",
    "What we don't need:\n",
    "- `notes`\n",
    "- `workout_notes`\n",
    "- `exercise_name`\n",
    "- `workout_name`\n",
    "- `exercise_normalized`\n",
    "- `distance` (cardio rows can be dropped entirely)\n",
    "- `seconds`\n",
    "\n",
    "\n",
    "## Now for some Feature Engineering Brainstorming!\n",
    "\n",
    "**Time-based features** help the model learn patterns based on when the workout occurred relative to other sessions:\n",
    "\n",
    "- **`days_since_last_session`** (captures recovery periods between sessions)\n",
    "- **`days_since_first_workout`** (shows the user’s experience level)\n",
    "- **`session_number`** (tracks progression within each exercise)\n",
    "- **`rolling_avg_load_last_n_sessions`** (captures trends in workout intensity)\n",
    "- **`rolling_trend_load (slope)`** (measures whether the user’s intensity is improving or declining)\n",
    "\n",
    "\n",
    "**RPE (Rate of Perceived Exertion)** will be handled in the following steps:\n",
    "\n",
    "1. **Imputation**: Fill missing RPE values with the **median** RPE.\n",
    "2. **Binning**: RPE values will be categorized into three groups:\n",
    "   - **Low**: 0 - 5\n",
    "   - **Medium**: 6 - 7\n",
    "   - **High**: 8 - 10\n",
    "3. **Encoding**: Convert RPE categories into an ordinal feature to capture workout intensity.\n",
    "\n",
    "\n",
    "**Reps (Number of Repetitions)** will be binned as follows:\n",
    "\n",
    "- **Strength**: 1 - 5 reps (lower rep ranges focus on strength training)\n",
    "- **Hypertrophy**: 6 - 15 reps (moderate rep ranges for muscle growth)\n",
    "- **Endurance**: 15+ reps (higher rep ranges for endurance training)\n",
    "\n",
    "Binning reps helps capture the type of training being done and its impact on the top-set intensity.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    ">Fortunately the functions to handle these feature engineering ideas have been defined in DE_utils.py, so no need to code it out. Just run the below cell and view the MI score for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def feature_engineering_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main pipeline for feature engineering.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The raw DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    dfCopy = df.copy()\n",
    "\n",
    "\n",
    "    dfCopy = de_utils.add_session_number_per_exercise(dfCopy)\n",
    "    dfCopy = de_utils.add_time_features(dfCopy)\n",
    "    dfCopy = de_utils.add_days_since_last_workout(dfCopy)\n",
    "    dfCopy = de_utils.add_rolling_avg_load_last_n_sessions(dfCopy, n=3)\n",
    "    dfCopy = de_utils.add_rolling_trend_load(dfCopy, n=3)\n",
    "    dfCopy['rolling_trend_load'] = dfCopy['rolling_trend_load'].fillna(dfCopy['rolling_trend_load'].mean())\n",
    "\n",
    "    dfCopy = de_utils.handle_missing_rpe(dfCopy)\n",
    "    dfCopy = de_utils.bin_rpe(dfCopy)\n",
    "    dfCopy = de_utils.encode_rpe_ordinal(dfCopy)\n",
    "\n",
    "    # IMPORTANT: use dfCopy, not df, to ensure engineered columns are available\n",
    "    dfCopy = de_utils.filter_top_set_sessions(dfCopy)\n",
    "\n",
    "    # We can drop these columns since we already extracted useful features from them, and we get to keep our numerical features\n",
    "    dfCopy = dfCopy.drop(columns=['date', 'rpe', 'rpe_binned', 'workout_name', 'exercise_name', 'exercise_normalized'])\n",
    "  \n",
    "\n",
    "    return dfCopy\n",
    "\n",
    "\n",
    "\n",
    "df = feature_engineering_pipeline(df)\n",
    "\n",
    "X = df.drop(columns=['effective_load'])\n",
    "y = df['effective_load']\n",
    "\n",
    "mi_scores = de_utils.get_mi_scores(X, y)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMutual Information Scores:\")\n",
    "print(mi_scores.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc98f84",
   "metadata": {},
   "source": [
    "### **Mutual Information Scores**\n",
    "\n",
    "After calculating the MI scores between features and the target (`effective_load`), we see the MI scores for each feature.\n",
    "\n",
    "\n",
    "### **Feature Selection Strategy**\n",
    "\n",
    "We'll drop features with an MI score below **0.15**, as they offer minimal value for predicting `effective_load`.\n",
    "\n",
    "#### **Features to Drop**:\n",
    "- **days_since_last_workout** (MI = 0.000)\n",
    "- **rpe_ordinal** (MI = 0.013)\n",
    "- **is_top_set** (MI = 0.059)\n",
    "\n",
    "#### **Remaining Features**:\n",
    "- **weight**\n",
    "- **rolling_avg_load_last_3_sessions**\n",
    "- **set_volume**\n",
    "- **rolling_trend_load**\n",
    "- **days_since_first_workout**\n",
    "- **rpe_missing**\n",
    "- **session_number**\n",
    "- **reps**\n",
    "- **set_order**\n",
    "\n",
    "\n",
    "### However...\n",
    "`rpe_missing` has way too high of an MI when it's not even that high of a predictor. That means rpe_missing is being inflated and the model will end up memorizing \"noise\". So, it's better to **drop** rpe_missing.\n",
    "\n",
    "\n",
    "The remaining features will be kept for model training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
